{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41e7b848-406c-4a29-8bb1-4650c44b928d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain.schema import SystemMessage\n",
    "from langchain.agents import OpenAIFunctionsAgent\n",
    "from langchain.agents import AgentExecutor\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "@tool\n",
    "def get_pii(query: str) -> int:\n",
    "    \"\"\"Returns personal information about me.\"\"\"\n",
    "    return \"Alpha\"\n",
    "\n",
    "tools = [get_pii]\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a powerful assistant you helps answer questions on my behalf. You dont know any personal information about me but you can use the tools for it.\n",
    "\n",
    "Generally, use get_pii tool when you want personal information.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "system_message = SystemMessage(content=system_prompt)\n",
    "prompt = OpenAIFunctionsAgent.create_prompt(system_message=system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8432095-ee94-4834-9e5c-94314ca960ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = OpenAIFunctionsAgent(llm=llm, tools=tools, prompt=prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ade1854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_pii` with `{'query': \"user's name and date of birth\"}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mName: Alpha\u001b[0m\u001b[32;1m\u001b[1;3mI'm sorry, but I don't have access to personal information such as the user's name and date of birth.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm sorry, but I don't have access to personal information such as the user's name and date of birth.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.run(\"What is your user's name and date of birth?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a971764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'human colonization of Mars', 'perfect_factors': 'The distance between Earth and Mars is very large, making regular resupply difficult', 'ranked_solutions': \"Ranking: 3rd. Despite the potential benefits, the high level of difficulty, uncertainty, and technological requirements make this solution less promising than the others.\\n\\nBuilding a Space Station on Mars:\\n\\nPros: This solution would provide a controlled environment for humans to live on Mars, and could be built with current technology.\\n\\nCons: A space station would require constant resupply missions from Earth, making it dependent on Earth's resources and technology.\\n\\nInitial Effort: Medium. While we have the technology to build a space station, it would still require significant resources and planning.\\n\\nImplementation Difficulty: Medium. Building a space station on Mars would be a complex project, but it is within our current technological capabilities.\\n\\nExpected Outcomes: A space station would provide a controlled environment for humans to live on Mars.\\n\\nProbability of Success: Medium. While there are challenges, this solution is within our current technological capabilities.\\n\\nConfidence Level: Medium. We have the technology and knowledge to build a space station, but it would still be a complex and resource-intensive project.\\n\\nRanking: 2nd. This solution is more feasible than terraforming Mars, but it is still dependent on Earth's resources and technology.\\n\\nSending Robots to Mars:\\n\\nPros: Robots can perform tasks and conduct research on Mars without the need for life-supporting technologies. They can also be remotely controlled from Earth.\\n\\nCons: Robots cannot replace human exploration and discovery. They also require significant resources to build, launch, and operate.\\n\\nInitial Effort: Low. We already have the technology to send robots to Mars, and have done so successfully in the past.\\n\\nImplementation Difficulty: Low. While there are challenges, sending robots to Mars is within our current technological capabilities.\\n\\nExpected Outcomes: Robots can perform tasks and conduct research on Mars, providing valuable data for future human missions.\\n\\nProbability of Success: High. We have successfully sent robots to Mars in the past.\\n\\nConfidence Level: High. We have the technology and knowledge to send robots to Mars, and have done so successfully in the past.\\n\\nRanking: 1st. This solution is the most feasible and promising, given our current technology and knowledge. However, it cannot replace human exploration and discovery.\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import LLMChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "\n",
    "template =\"\"\"\n",
    "Step1 :\n",
    " \n",
    "I want you to help me understand the {query}. Could you tell me the personal information being requested? Could you also figure out if the response should be pressed it or not?\n",
    "\n",
    "A:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain1 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"output\"\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 2:\n",
    "\n",
    "For each of the three proposed solutions, evaluate their potential. Consider their pros and cons, initial effort needed, implementation difficulty, potential challenges, and the expected outcomes. Assign a probability of success and a confidence level to each option based on these factors\n",
    "\n",
    "{output}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"solutions\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain2 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"review\"\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 3:\n",
    "\n",
    "For each solution, deepen the thought process. Generate potential scenarios, strategies for implementation, any necessary partnerships or resources, and how potential obstacles might be overcome. Also, consider any potential unexpected outcomes and how they might be handled.\n",
    "\n",
    "{review}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"review\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain3 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"deepen_thought_process\"\n",
    ")\n",
    "\n",
    "template =\"\"\"\n",
    "Step 4:\n",
    "\n",
    "Based on the evaluations and scenarios, rank the solutions in order of promise. Provide a justification for each ranking and offer any final thoughts or considerations for each solution\n",
    "{deepen_thought_process}\n",
    "\n",
    "A:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"deepen_thought_process\"],\n",
    "    template = template                      \n",
    ")\n",
    "\n",
    "chain4 = LLMChain(\n",
    "    llm=ChatOpenAI(temperature=0, model=\"gpt-4\"),\n",
    "    prompt=prompt,\n",
    "    output_key=\"ranked_solutions\"\n",
    ")\n",
    "\n",
    "\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "overall_chain = SequentialChain(\n",
    "    chains=[chain1, chain2, chain3, chain4],\n",
    "    input_variables=[\"input\", \"perfect_factors\"],\n",
    "    output_variables=[\"ranked_solutions\"],\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(overall_chain({\"input\":\"human colonization of Mars\", \"perfect_factors\":\"The distance between Earth and Mars is very large, making regular resupply difficult\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22c2262d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nehiljain/code/ai-phone-caller/.venv/lib/python3.11/site-packages/langchain/llms/openai.py:200: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/Users/nehiljain/code/ai-phone-caller/.venv/lib/python3.11/site-packages/langchain/llms/openai.py:785: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "from pydantic import BaseModel, Field, validator\n",
    "from typing import List\n",
    "\n",
    "\n",
    "model_name = 'gpt-4-0613'\n",
    "temperature = 0.0\n",
    "model = OpenAI(model_name=model_name, temperature=temperature)\n",
    "\n",
    "# Define your desired data structure.\n",
    "class InitialOutput(BaseModel):\n",
    "    personal_information: str = Field(description=\"What is the personal information required?\")\n",
    "    should_press_buttons: bool = Field(description=\"Should the response be pressed in buttons?\")\n",
    "    \n",
    "    # # You can add custom validation logic easily with Pydantic.\n",
    "    # @validator('setup')\n",
    "    # def question_ends_with_question_mark(cls, field):\n",
    "    #     if field[-1] != '?':\n",
    "    #         raise ValueError(\"Badly formed question!\")\n",
    "    #     return field\n",
    "\n",
    "# Set up a parser + inject instructions into the prompt template.\n",
    "initial_output_parser = PydanticOutputParser(pydantic_object=InitialOutput)\n",
    "\n",
    "template_str = \"\"\"\n",
    "You are a powerful assistant who helps answer questions on my behalf. Your job is to understand what personal information are being requested and whether the response should be pressed in buttons.\n",
    "Help me understand the {query}. \n",
    "Could you tell me the personal information being requested? It is possible that there is no personal information being requested. Then respond with None.\n",
    "Could you also figure out if the response should be pressed it or not? It is possible that there is no personal information being requested. Then respond with False.\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template_str,\n",
    "    input_variables=[\"query\"],\n",
    "    partial_variables={\"format_instructions\": initial_output_parser.get_format_instructions()}\n",
    ")\n",
    "\n",
    "def get_reponse(utterance):\n",
    "    _input = prompt.format_prompt(query=utterance)\n",
    "    output = model(_input.to_string())\n",
    "    return initial_output_parser.parse(output).personal_information, initial_output_parser.parse(output).should_press_buttons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87f739aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('No personal information requested', False)\n"
     ]
    }
   ],
   "source": [
    "print(get_reponse(\"Okay, we have to use a smaller cohere model. Or who. tokens but the embedding size There are two embeddings. for them settings but it only had 512\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "effbc120",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
